{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy import linalg\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.models import vgg19\n",
    "from PIL import Image\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/srgan/models.py\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = vgg19(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.feature_extractor(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data set into two different loader with different resolution one is low resolution as an input to generator 64 * 64 and other as HR image of size 256 * 256.\n",
    "We will assume 256 * 256 as HR image instead of original image as the size of original image are two big for the resource I currently have\n",
    "Batch size is also just two images at a time due to memory contraints, going over 2 image primary memory over flow due to the sizing of generator's convolution.\n",
    "DataSet used for training is downloaded from \n",
    "https://data.vision.ee.ethz.ch/cvl/DIV2K/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 2\n",
    "transform_low=transforms.Compose([\n",
    "                               transforms.Resize((64,64),Image.BICUBIC),\n",
    "                               transforms.ToTensor(),\n",
    "                               ])\n",
    "transform_High=transforms.Compose([\n",
    "                               transforms.Resize((256,256),Image.BICUBIC),\n",
    "                               transforms.ToTensor(),\n",
    "                               ])\n",
    "\n",
    "# transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n",
    "lowResSet = dset.ImageFolder(root = 'C:\\\\Users\\\\saura\\\\Downloads\\\\DIV2K_train_HR', transform = transform_low) \n",
    "highResSet = dset.ImageFolder(root = 'C:\\\\Users\\\\saura\\\\Downloads\\\\DIV2K_train_HR', transform = transform_High) \n",
    "lowResLoader = torch.utils.data.DataLoader(lowResSet, batch_size = batchSize, num_workers = 2)\n",
    "highResLoader = torch.utils.data.DataLoader(highResSet, batch_size = batchSize, num_workers = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Images as grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveFigGrid(input_imgs, name):\n",
    "    imgs =  torchvision.utils.make_grid(input_imgs)\n",
    "    npImgs = imgs.numpy()\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.imshow(np.transpose(npImgs, (1,2,0)), cmap = 'Greys_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    fig.savefig(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the loader, dataset are working correctly by savinf some sample in 8 by 8 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIter  =  iter(highResLoader)\n",
    "imgs, labels = trainIter.next()\n",
    "imgs.shape\n",
    "# SaveFigGrid(imgs,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "            super(G, self).__init__()\n",
    "            self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, 9, 1, 4, bias = False),\n",
    "                nn.PReLU(),\n",
    "            )\n",
    "            \n",
    "            self.layer2 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(64, 64, 3, 1, 1, bias = False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(64, 64, 3, 1, 1, bias = False),\n",
    "                nn.BatchNorm2d(64),\n",
    "            )\n",
    "            \n",
    "            self.layer3 = nn.Sequential(\n",
    "                nn.Conv2d(64, 64, 3, 1, 1, bias = False),\n",
    "                nn.BatchNorm2d(64),\n",
    "            )\n",
    "            \n",
    "            self.layer4 = nn.Sequential(\n",
    "                nn.Conv2d(64, 256, 3, 1, 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.PixelShuffle(upscale_factor=2),\n",
    "                nn.PReLU(),\n",
    "                nn.Conv2d(64, 256, 3, 1, 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.PixelShuffle(upscale_factor=2),\n",
    "                nn.PReLU(),\n",
    "            )\n",
    "            \n",
    "            self.layer5 = nn.Sequential(\n",
    "                nn.Conv2d(64, 3, 9,1,4),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "                \n",
    "    def forward(self, x):\n",
    "            out1 = self.layer1(x)\n",
    "            out = out1\n",
    "            for _ in range(18):\n",
    "                out = self.layer2(out)\n",
    "            out2 = self.layer3(out)\n",
    "            out = torch.add(out1,out2)\n",
    "            out = self.layer4(out)\n",
    "            out = self.layer5(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = G()\n",
    "# netG(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriminator Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "            super(D, self).__init__()\n",
    "            \n",
    "            self.model = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, 3, 1, 1),\n",
    "                nn.LeakyReLU(0.1, inplace = True),\n",
    "                nn.Conv2d(64, 64, 3,2,1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                \n",
    "                nn.Conv2d(64, 128, 3, 1, 1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(0.1, inplace = True),\n",
    "                nn.Conv2d(128, 128,3,2,1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                \n",
    "                nn.Conv2d(128, 256, 3, 1, 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1, inplace = True),\n",
    "                nn.Conv2d(256, 256,3,2,1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                \n",
    "                nn.Conv2d(256, 512, 3, 1, 1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1, inplace = True),\n",
    "                nn.Conv2d(512, 512, 3,2,1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                \n",
    "                nn.Conv2d(512, 1, 4, 1, 1),\n",
    "                nn.Sigmoid(),\n",
    "                \n",
    "                \n",
    "            ) \n",
    "                \n",
    "    def forward(self, x):\n",
    "            return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = D()\n",
    "flag = False     # turn this flag as true if you want to load previously saved models\n",
    "# netD(imgs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag:\n",
    "    checkpoint = torch.load(\"srgan/models/model_20_iteration_9399.pth\")\n",
    "    netG.load_state_dict(checkpoint['G_State'])\n",
    "    netD.load_state_dict(checkpoint['D_State'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers for Generator and Discriminator(TTUR) and Loss function as Binary Cross Entrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "criterion_content = nn.MSELoss()\n",
    "optD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optG = optim.Adam(netG.parameters(), lr = 0.0001, betas = (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(name, generator, discriminator):\n",
    "    torch.save({\n",
    "                'G': generator,\n",
    "                'D': discriminator,\n",
    "                'G_State': generator.state_dict(),\n",
    "                'D_State': discriminator.state_dict()\n",
    "               }, name+\".pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tb = SummaryWriter(\"srgan/run4\")\n",
    "i = -1\n",
    "for epoch in range(100):\n",
    "    print(\"epoch\", epoch)\n",
    "    lowIter  =  iter(lowResLoader)\n",
    "    highIter =  iter(highResLoader)\n",
    "    for _ in range(len(lowIter)):\n",
    "        i += 1\n",
    "        optG.zero_grad()\n",
    "        low, labelLow = lowIter.next()\n",
    "        high, labelHigh = highIter.next()\n",
    "        low = Variable(low)\n",
    "        high = Variable(high)\n",
    "        SR = netG(low)\n",
    "        desSR = netD(SR.detach())\n",
    "        true_labels = torch.ones(desSR.shape)\n",
    "        adv_loss = criterion(desSR,true_labels)\n",
    "        \n",
    "        #content Loss\n",
    "        sr_feature = feature_extractor(SR)\n",
    "        hr_feature = feature_extractor(high)\n",
    "        content_loss = criterion_content(sr_feature,hr_feature.detach())\n",
    "        \n",
    "#         https://medium.com/@jonathan_hui/gan-super-resolution-gan-srgan-b471da7270ec\n",
    "        lossG = (1e-3 * content_loss) + adv_loss\n",
    "        lossG.backward()\n",
    "        optG.step()\n",
    "        \n",
    "        optD.zero_grad()\n",
    "        false_labels = torch.zeros(desSR.shape)\n",
    "        desHR = netD(high)\n",
    "        desSR = netD(SR.detach())\n",
    "        loss_real_des = criterion(desHR,true_labels)\n",
    "        loss_fake_des = criterion(desSR,false_labels)\n",
    "        lossD = (loss_real_des + loss_fake_des)\n",
    "        lossD.backward()\n",
    "        optD.step()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            SaveFigGrid(SR.detach(), \"srgan/SR/epoch_\"+str(epoch)+\"_iteration_\"+str(i))\n",
    "            SaveFigGrid(high, \"srgan/HR/epoch_\"+str(epoch)+\"_iteration_\"+str(i))\n",
    "            SaveFigGrid(low, \"srgan/LR/epoch_\"+str(epoch)+\"_iteration_\"+str(i))\n",
    "            save_model(\"srgan/models/model_\"+str(epoch)+\"_iteration_\"+str(i), netG, netD)   \n",
    "    \n",
    "    tb.add_scalar(\"Adversarial_loss\", adv_loss, epoch)\n",
    "    tb.add_scalar(\"content_loss\", content_loss, epoch)\n",
    "    tb.add_scalar(\"Total_Gen_Loss\", lossG, epoch)\n",
    "    tb.add_scalar(\"Total_Des_Loss\", lossD, epoch)\n",
    "    tb.add_scalars(f'Discriminator loss vs Generator loss', {\n",
    "                    'Discriminator loss': lossD,\n",
    "                    'Generator loss': lossG,\n",
    "                    }, epoch)\n",
    "    tb.add_scalars(f'Adversial loss vs content loss', {\n",
    "                    'Adversial loss': lossD,\n",
    "                    'Generator loss': lossG,\n",
    "                    }, epoch)\n",
    "    \n",
    "    \n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage.measure import compare_psnr as psnr\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated all the Super resolution image from BSD dataset and save them to disk for calculating SSIM and PSNR\n",
    "BSD100 dataset is downloaded from https://github.com/jbhuang0604/SelfExSR/tree/master/data/BSD100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = G()\n",
    "checkpoint = torch.load(\"srgan/models/model_20_iteration_9399.pth\")\n",
    "netG.load_state_dict(checkpoint['G_State'])\n",
    "\n",
    "\n",
    "def SaveFig(input_imgs, name):\n",
    "    npImgs = input_imgs.numpy()[0]\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    plt.imshow(np.transpose(npImgs, (1,2,0)))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    fig.savefig(name)\n",
    "\n",
    "\n",
    "\n",
    "batch = 1\n",
    "low_trans=transforms.Compose([\n",
    "                               transforms.Resize((64,64),Image.BICUBIC),\n",
    "                               transforms.ToTensor(),\n",
    "                               ])\n",
    "lowResValSet = dset.ImageFolder(root = 'C:\\\\Users\\\\saura\\\\Deep Learning\\\\Proj 2\\\\valImage', transform = low_trans) \n",
    "lowResValLoader = torch.utils.data.DataLoader(lowResValSet, batch_size = batch)\n",
    "valIter  =  iter(lowResValLoader)\n",
    "\n",
    "\n",
    "\n",
    "path = \"C:\\\\Users\\\\saura\\\\Deep Learning\\\\Proj 2\\\\valImage\\\\BSD\"\n",
    "dirs = os.listdir( path )\n",
    "for item in dirs:\n",
    "    low, labelLow = valIter.next()\n",
    "    low = Variable(low)\n",
    "    SR = netG(low)\n",
    "    SaveFig(SR.detach(), \"C:\\\\Users\\\\saura\\\\Deep Learning\\\\Proj 2\\\\valImage\\\\SR\\\\\"+item)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate SSIM and PSNR, Please set the correct path for both type of image set before calucation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathH = \"C:\\\\Users\\\\saura\\\\Deep Learning\\\\Proj 2\\\\valImage\\\\BSD\"\n",
    "pathL = \"C:\\\\Users\\\\saura\\\\Deep Learning\\\\Proj 2\\\\valImage\\\\SR\"\n",
    "dirs = os.listdir( path )\n",
    "\n",
    "tb = SummaryWriter(\"srgan/runVal\")\n",
    "i = -1\n",
    "ssimT = 0\n",
    "psnrT = 0\n",
    "for item in dirs:\n",
    "    i +=1\n",
    "    im = Image.open(pathH+\"\\\\\"+item)\n",
    "    imH = np.asarray(im.resize((256,256), Image.BICUBIC).convert('LA'))\n",
    "    im = Image.open(pathL+\"\\\\\"+item)\n",
    "    imL = np.asarray(im.resize((256,256), Image.BICUBIC).convert('LA'))\n",
    "    ssimVal = ssim(imH,imL, multichannel = True)\n",
    "    psnrVal = psnr(imH,imL)\n",
    "    ssimT += ssimVal\n",
    "    psnrT += psnrVal\n",
    "    tb.add_scalar(\"ssim\", ssimVal, i)\n",
    "    tb.add_scalar(\"psnr\", psnrVal, i)\n",
    "\n",
    "tb.add_scalar(\"average_ssim\", ssimT/(i+1))\n",
    "tb.add_scalar(\"average_psnr\", psnrT/(i+1))\n",
    "tb.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
